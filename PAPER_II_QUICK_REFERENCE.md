# Paper II: Statistics and Computer Applications
## Quick Reference Guide for Theory Exam

---

## ğŸ“Š Course Objective
**To equip scholars with statistical tools and software skills for research data analysis.**

---

## ğŸ“š UNIT I: Statistical Foundations

### 1. Hypothesis Design

**Null Hypothesis (Hâ‚€):**
- Statement of "no effect" or "no difference"
- What we test against
- Example: Hâ‚€: Î¼â‚ = Î¼â‚‚ (no difference between groups)

**Alternative Hypothesis (Hâ‚):**
- Statement of effect or difference
- What we accept if we reject Hâ‚€
- Types:
  - **Two-tailed**: Hâ‚: Î¼â‚ â‰  Î¼â‚‚ (different, direction unknown)
  - **One-tailed (right)**: Hâ‚: Î¼â‚ > Î¼â‚‚ (greater than)
  - **One-tailed (left)**: Hâ‚: Î¼â‚ < Î¼â‚‚ (less than)

**Hypothesis Testing Steps:**
1. State Hâ‚€ and Hâ‚
2. Choose significance level (Î± = 0.05 typically)
3. Select appropriate test
4. Calculate test statistic
5. Find critical value or p-value
6. Make decision (reject or fail to reject Hâ‚€)
7. State conclusion in context

### 2. Parametric vs Non-Parametric Tests

| Aspect | Parametric | Non-Parametric |
|--------|------------|----------------|
| **Assumptions** | Normal distribution, interval/ratio data | No distribution assumption, any data type |
| **Data Type** | Interval, Ratio | Nominal, Ordinal, Non-normal |
| **Power** | Higher (if assumptions met) | Lower |
| **Examples** | t-test, ANOVA, Pearson r | Mann-Whitney, Kruskal-Wallis, Spearman |

### 3. Parametric Tests

#### **z-Test**
- **When**: Large sample (n > 30), population Ïƒ known
- **Formula**: z = (xÌ„ - Î¼â‚€) / (Ïƒ/âˆšn)
- **Critical value**: Â±1.96 (Î± = 0.05, two-tailed)
- **Decision**: If |z| > 1.96, reject Hâ‚€

#### **t-Test**

**One-Sample t-test:**
- **When**: Compare sample mean to population mean, Ïƒ unknown
- **Formula**: t = (xÌ„ - Î¼â‚€) / (s/âˆšn)
- **df**: n - 1

**Independent Samples t-test:**
- **When**: Compare means of two independent groups
- **Formula**: t = (xÌ„â‚ - xÌ„â‚‚) / âˆš(sÂ²pooled(1/nâ‚ + 1/nâ‚‚))
- **df**: nâ‚ + nâ‚‚ - 2

**Paired t-test:**
- **When**: Compare means of same group at two times
- **Formula**: t = dÌ„ / (sd/âˆšn)
- **df**: n - 1 (n = number of pairs)

**t-test Assumptions:**
- âœ… Continuous data (interval/ratio)
- âœ… Normal distribution (or n > 30)
- âœ… Independent observations
- âœ… Equal variances (for independent samples)

#### **ANOVA (Analysis of Variance)**
- **When**: Compare means of 3+ groups
- **Null**: Hâ‚€: Î¼â‚ = Î¼â‚‚ = Î¼â‚ƒ = ... (all means equal)
- **Test Statistic**: F = MSbetween / MSwithin
- **df**: df_between = k - 1, df_within = N - k
- **Decision**: If F > F_critical, reject Hâ‚€ (at least one mean differs)

**ANOVA Table:**
| Source | SS | df | MS | F |
|--------|----|----|----|----|
| Between Groups | SSB | k-1 | MSB = SSB/(k-1) | F = MSB/MSW |
| Within Groups | SSW | N-k | MSW = SSW/(N-k) | |
| Total | SST | N-1 | | |

**Post-hoc Tests** (if ANOVA significant):
- Tukey HSD
- Bonferroni
- ScheffÃ©

### 4. Degrees of Freedom (df)

**Quick Reference:**
| Test | df Formula |
|------|-----------|
| One-sample t-test | n - 1 |
| Independent t-test | nâ‚ + nâ‚‚ - 2 |
| Paired t-test | n - 1 |
| Chi-square | (rows - 1) Ã— (columns - 1) |
| ANOVA (between) | k - 1 |
| ANOVA (within) | N - k |

### 5. Confidence Intervals

**Formula**: CI = xÌ„ Â± (critical value Ã— SE)

**For mean**: CI = xÌ„ Â± t(Î±/2, df) Ã— (s/âˆšn)

**Common Levels:**
| Confidence Level | z-value | Interpretation |
|-----------------|---------|----------------|
| 90% | 1.645 | 90% confident true mean is in interval |
| 95% | 1.96 | 95% confident true mean is in interval |
| 99% | 2.576 | 99% confident true mean is in interval |

### 6. Non-Parametric Tests

#### **Kolmogorov-Smirnov Test**
- **Purpose**: Test if sample follows a specific distribution
- **When**: Check normality assumption
- **Hâ‚€**: Sample follows specified distribution
- **Alternative**: Shapiro-Wilk test (better for small samples)

#### **Run Test**
- **Purpose**: Test randomness of a sequence
- **When**: Check if data is random or has pattern
- **Application**: Time series, quality control

#### **Mann-Whitney U Test**
- **Purpose**: Non-parametric alternative to independent t-test
- **When**: Ordinal data or non-normal distributions
- **Hâ‚€**: Two groups have same distribution
- **Effect size**: r = Z/âˆšN

#### **Kruskal-Wallis Test**
- **Purpose**: Non-parametric alternative to one-way ANOVA
- **When**: Ordinal data or non-normal, 3+ groups
- **Hâ‚€**: All groups have same distribution
- **Post-hoc**: Dunn's test

#### **Chi-Square (Ï‡Â²) Test**

**Goodness of Fit:**
- **Purpose**: Test if observed frequencies match expected
- **Formula**: Ï‡Â² = Î£[(O - E)Â²/E]
- **df**: k - 1 (k = number of categories)

**Test of Independence:**
- **Purpose**: Test if two categorical variables are related
- **Formula**: Ï‡Â² = Î£[(O - E)Â²/E]
- **df**: (rows - 1) Ã— (columns - 1)
- **Effect size**: CramÃ©r's V

**Expected Frequency**: E = (row total Ã— column total) / grand total

**Assumption**: All expected frequencies â‰¥ 5

### 7. Type I and Type II Errors

| Reality | Hâ‚€ True | Hâ‚€ False |
|---------|---------|----------|
| **Reject Hâ‚€** | Type I Error (Î±) | Correct (Power = 1-Î²) |
| **Fail to Reject Hâ‚€** | Correct | Type II Error (Î²) |

- **Type I Error (Î±)**: False Positive - Reject true Hâ‚€
- **Type II Error (Î²)**: False Negative - Fail to reject false Hâ‚€
- **Power (1-Î²)**: Probability of correctly rejecting false Hâ‚€

**Typical Î± = 0.05** (5% chance of Type I error)

---

## ğŸ“ˆ UNIT II: Correlation & Regression

### 1. Charts and Tables

**Chart Selection:**
| Data Type | Best Chart |
|-----------|-----------|
| Continuous vs Continuous | Scatter plot |
| Categorical vs Continuous | Bar chart, Box plot |
| Distribution | Histogram |
| Time series | Line chart |
| Proportions | Pie chart |

### 2. Correlation Analysis

#### **Pearson Correlation (r)**
- **Range**: -1 to +1
- **Interpretation**:
  - r = +1: Perfect positive correlation
  - r = 0: No correlation
  - r = -1: Perfect negative correlation
  
**Strength:**
| |r| | Strength |
|------|----------|
| 0.00-0.19 | Very weak |
| 0.20-0.39 | Weak |
| 0.40-0.59 | Moderate |
| 0.60-0.79 | Strong |
| 0.80-1.00 | Very strong |

**Formula**: r = Î£[(x - xÌ„)(y - È³)] / âˆš[Î£(x - xÌ„)Â² Ã— Î£(y - È³)Â²]

**Assumptions:**
- âœ… Linear relationship
- âœ… Continuous variables
- âœ… Normal distribution
- âœ… No outliers

**rÂ² (Coefficient of Determination):**
- Proportion of variance explained
- Example: r = 0.7 â†’ rÂ² = 0.49 (49% variance explained)

#### **Spearman's Rank Correlation (Ï)**
- **When**: Ordinal data or non-linear relationship
- **Range**: -1 to +1
- **Method**: Rank data, then calculate Pearson on ranks

### 3. Regression Analysis

#### **Simple Linear Regression**

**Equation**: Å· = a + bx
- Å· = predicted value
- a = y-intercept
- b = slope
- x = predictor variable

**Least Squares Method:**
- **Slope**: b = Î£[(x - xÌ„)(y - È³)] / Î£(x - xÌ„)Â²
- **Intercept**: a = È³ - b Ã— xÌ„

**Interpretation:**
- **Slope (b)**: For every 1-unit increase in x, y changes by b units
- **Intercept (a)**: Value of y when x = 0

**RÂ² (Coefficient of Determination):**
- Proportion of variance in y explained by x
- Range: 0 to 1
- Higher is better (but watch for overfitting)

**Adjusted RÂ²:**
- Adjusts for number of predictors
- Use for comparing models

### 4. Properties and Assumptions of Regression

**Classical Assumptions:**
1. **Linearity**: Relationship is linear
2. **Independence**: Observations are independent
3. **Homoscedasticity**: Constant variance of errors
4. **Normality**: Errors are normally distributed
5. **No multicollinearity**: Predictors not highly correlated (multiple regression)

**Checking Assumptions:**
- Linearity: Scatter plot
- Normality: Histogram of residuals, Q-Q plot
- Homoscedasticity: Residual plot (residuals vs fitted values)
- Independence: Durbin-Watson test

### 5. Gauss-Markov Theorem

**Statement**: Under classical assumptions, OLS (Ordinary Least Squares) estimators are BLUE:
- **B**est: Minimum variance
- **L**inear: Linear function of y
- **U**nbiased: E(b) = Î²
- **E**stimator: Estimates population parameter

**Implication**: OLS is the optimal method for linear regression when assumptions are met.

### 6. Multiple Regression

**Equation**: Å· = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚–xâ‚–

**Interpretation:**
- **bâ‚**: Change in y for 1-unit change in xâ‚, holding other variables constant

**Model Fit:**
- **RÂ²**: Proportion of variance explained
- **Adjusted RÂ²**: Penalizes for number of predictors
- **F-test**: Overall model significance

**Multicollinearity:**
- **Problem**: Predictors highly correlated
- **Detection**: VIF (Variance Inflation Factor) > 10
- **Solution**: Remove correlated predictors

**Standardized Coefficients (Î²):**
- Allows comparison of relative importance
- All variables on same scale

---

## ğŸ’» UNIT III: Computer Applications

### 1. MS Excel

**Essential Formulas:**
| Function | Purpose | Example |
|----------|---------|---------|
| =AVERAGE() | Mean | =AVERAGE(A1:A10) |
| =STDEV.S() | Sample SD | =STDEV.S(A1:A10) |
| =CORREL() | Correlation | =CORREL(A1:A10, B1:B10) |
| =TTEST() | t-test | =TTEST(A1:A10, B1:B10, 2, 2) |
| =VLOOKUP() | Lookup value | =VLOOKUP(value, range, col, 0) |

**Data Analysis ToolPak:**
- Descriptive Statistics
- t-Test (all types)
- ANOVA
- Regression
- Correlation

**Charts:**
- Insert â†’ Charts â†’ Select type
- Scatter plot for correlation
- Histogram for distribution
- Box plot for group comparison

### 2. SPSS

**Interface:**
- **Data View**: Enter data
- **Variable View**: Define variables (name, type, measure)

**Measurement Levels:**
- **Scale**: Continuous (interval/ratio)
- **Ordinal**: Ordered categories
- **Nominal**: Categories

**Common Analyses:**
| Analysis | Menu Path |
|----------|-----------|
| Descriptives | Analyze â†’ Descriptive Statistics â†’ Descriptives |
| t-test | Analyze â†’ Compare Means â†’ Independent-Samples T Test |
| ANOVA | Analyze â†’ Compare Means â†’ One-Way ANOVA |
| Correlation | Analyze â†’ Correlate â†’ Bivariate |
| Regression | Analyze â†’ Regression â†’ Linear |
| Chi-square | Analyze â†’ Descriptive Statistics â†’ Crosstabs |

**Output Interpretation:**
- **Sig. (p-value)**: If < 0.05, significant
- **t or F value**: Test statistic
- **df**: Degrees of freedom
- **Mean Difference**: Difference between groups

**APA Reporting from SPSS:**
```
t(df) = value, p = .xxx
F(df1, df2) = value, p = .xxx
r = .xx, p = .xxx
```

### 3. R Programming

**Installation:**
1. Install R from r-project.org
2. Install RStudio from rstudio.com

**Basic Commands:**
```r
# Read data
data <- read.csv("file.csv")

# Descriptive statistics
mean(data$variable)
sd(data$variable)
summary(data)

# t-test
t.test(score ~ group, data = data)

# ANOVA
model <- aov(score ~ treatment, data = data)
summary(model)

# Correlation
cor.test(data$x, data$y)

# Regression
model <- lm(y ~ x, data = data)
summary(model)

# Multiple regression
model <- lm(y ~ x1 + x2 + x3, data = data)
summary(model)
```

**Essential Packages:**
```r
install.packages("tidyverse")  # Data manipulation
install.packages("ggplot2")    # Visualization
install.packages("psych")      # Psychology stats
install.packages("car")        # Regression diagnostics
```

**Visualization:**
```r
library(ggplot2)

# Scatter plot
ggplot(data, aes(x = x, y = y)) + geom_point()

# Box plot
ggplot(data, aes(x = group, y = score)) + geom_boxplot()

# Histogram
ggplot(data, aes(x = score)) + geom_histogram()
```

### 4. MATLAB

**Basic Operations:**
```matlab
% Descriptive statistics
mean(data)
std(data)
var(data)

% t-test
[h, p, ci, stats] = ttest(sample1, sample2)

% ANOVA
[p, tbl, stats] = anova1(data, group)

% Correlation
[r, p] = corrcoef(x, y)

% Regression
mdl = fitlm(x, y)
```

**Plotting:**
```matlab
% Scatter plot
scatter(x, y)

% Histogram
histogram(data)

% Box plot
boxplot(data, group)
```

### 5. LaTeX

**Basic Document:**
```latex
\documentclass{article}
\usepackage{amsmath}  % For equations
\usepackage{graphicx} % For figures

\begin{document}

\title{Research Paper}
\author{Your Name}
\maketitle

\section{Introduction}
Text here.

% Equation
\begin{equation}
y = \beta_0 + \beta_1 x + \epsilon
\end{equation}

% Table
\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
Variable & Mean \\
\hline
X & 10.5 \\
Y & 20.3 \\
\hline
\end{tabular}
\caption{Descriptive Statistics}
\end{table}

% Figure
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{plot.png}
\caption{Scatter Plot}
\end{figure}

\end{document}
```

### 6. ATLAS.ti

**Purpose**: Qualitative data analysis

**Workflow:**
1. Import documents (text, audio, video)
2. Create codes
3. Code segments of data
4. Create memos
5. Build networks
6. Generate reports

### 7. AMOS

**Purpose**: Structural Equation Modeling (SEM)

**Components:**
- **Observed variables**: Measured directly (rectangles)
- **Latent variables**: Not directly measured (ovals)
- **Paths**: Relationships (arrows)

**Model Fit Indices:**
- **Ï‡Â²/df**: < 3 good
- **CFI**: > 0.95 good
- **RMSEA**: < 0.06 good
- **SRMR**: < 0.08 good

---

## ğŸ¯ Test Selection Flowchart

```
What type of data?
â”œâ”€ Continuous (Interval/Ratio)
â”‚   â”œâ”€ Normal distribution?
â”‚   â”‚   â”œâ”€ Yes â†’ Parametric tests
â”‚   â”‚   â”‚   â”œâ”€ Compare to population â†’ z-test (n>30) or t-test
â”‚   â”‚   â”‚   â”œâ”€ Compare 2 groups â†’ Independent t-test
â”‚   â”‚   â”‚   â”œâ”€ Compare same group twice â†’ Paired t-test
â”‚   â”‚   â”‚   â””â”€ Compare 3+ groups â†’ ANOVA
â”‚   â”‚   â””â”€ No â†’ Non-parametric tests
â”‚   â”‚       â”œâ”€ Compare 2 groups â†’ Mann-Whitney U
â”‚   â”‚       â””â”€ Compare 3+ groups â†’ Kruskal-Wallis
â”‚   â””â”€ Relationship between variables?
â”‚       â”œâ”€ Linear â†’ Pearson correlation, Regression
â”‚       â””â”€ Non-linear â†’ Spearman correlation
â””â”€ Categorical (Nominal/Ordinal)
    â”œâ”€ Ordinal â†’ Spearman correlation, Mann-Whitney, Kruskal-Wallis
    â””â”€ Nominal â†’ Chi-square test
```

---

## ğŸ“ Exam Preparation Checklist

### Formulas to Memorize:
- [ ] z-test formula
- [ ] t-test formula (all types)
- [ ] F-ratio for ANOVA
- [ ] Pearson correlation
- [ ] Regression slope and intercept
- [ ] Chi-square formula
- [ ] Confidence interval formula

### Concepts to Understand:
- [ ] When to use each test
- [ ] Assumptions of each test
- [ ] Interpretation of p-values
- [ ] Type I and Type II errors
- [ ] Degrees of freedom calculation
- [ ] RÂ² interpretation
- [ ] Gauss-Markov theorem

### Software Skills:
- [ ] Excel: Data Analysis ToolPak
- [ ] SPSS: Basic analyses and output interpretation
- [ ] R: Basic commands and syntax
- [ ] LaTeX: Document structure

### Practice:
- [ ] Work through example problems
- [ ] Interpret SPSS/R output
- [ ] Write results in APA format
- [ ] Select appropriate test for scenarios

---

**Good Luck with Your Statistics Exam! ğŸ“Š**

---

*Last Updated: 2026-01-15*
*For detailed explanations and examples, refer to the full course modules.*
